{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d09de67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model  Accuracy (%)  Precision (%)  Recall (%)  F1-Score (%)\n",
      "0          GaussianNB         76.62          66.10       70.91         68.42\n",
      "5  LogisticRegression         75.32          68.09       58.18         62.75\n",
      "9                 SVM         75.32          68.89       56.36         62.00\n",
      "6        DecisionTree         74.68          62.50       72.73         67.23\n",
      "4       CategoricalNB         73.38          63.46       60.00         61.68\n",
      "7        RandomForest         72.08          60.71       61.82         61.26\n",
      "8                 KNN         68.18          55.77       52.73         54.21\n",
      "2         BernoulliNB         64.94          50.85       54.55         52.63\n",
      "1       MultinomialNB         64.29           0.00        0.00          0.00\n",
      "3        ComplementNB         59.09          44.29       56.36         49.60\n",
      "\n",
      "Comparison saved to 'classifier_comparison_percentage.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, ComplementNB, CategoricalNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"diabetes.csv\")  # Change path if needed\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(\"Outcome\", axis=1)\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scaling for models that need it\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
    "\n",
    "# Binarized for BernoulliNB\n",
    "X_train_bin = (X_train_scaled > 0.5).astype(int)\n",
    "X_test_bin = (X_test_scaled > 0.5).astype(int)\n",
    "\n",
    "# Categorical for CategoricalNB\n",
    "X_train_cat = X_train_scaled.apply(lambda col: pd.qcut(col, q=4, labels=False, duplicates=\"drop\"))\n",
    "X_test_cat = X_test_scaled.apply(lambda col: pd.qcut(col, q=4, labels=False, duplicates=\"drop\"))\n",
    "\n",
    "# Classifiers to compare\n",
    "models = {\n",
    "    \"GaussianNB\": (GaussianNB(), X_train, X_test),\n",
    "    \"MultinomialNB\": (MultinomialNB(), X_train_scaled, X_test_scaled),\n",
    "    \"BernoulliNB\": (BernoulliNB(), X_train_bin, X_test_bin),\n",
    "    \"ComplementNB\": (ComplementNB(), X_train_scaled, X_test_scaled),\n",
    "    \"CategoricalNB\": (CategoricalNB(), X_train_cat, X_test_cat),\n",
    "    \"LogisticRegression\": (LogisticRegression(max_iter=1000), X_train_scaled, X_test_scaled),\n",
    "    \"DecisionTree\": (DecisionTreeClassifier(random_state=42), X_train, X_test),\n",
    "    \"RandomForest\": (RandomForestClassifier(random_state=42), X_train, X_test),\n",
    "    \"KNN\": (KNeighborsClassifier(), X_train_scaled, X_test_scaled),\n",
    "    \"SVM\": (SVC(), X_train_scaled, X_test_scaled)\n",
    "}\n",
    "\n",
    "# Store metrics\n",
    "results = []\n",
    "\n",
    "for name, (model, Xtr, Xte) in models.items():\n",
    "    model.fit(Xtr, y_train)\n",
    "    preds = model.predict(Xte)\n",
    "    acc = accuracy_score(y_test, preds) * 100\n",
    "    prec = precision_score(y_test, preds) * 100\n",
    "    rec = recall_score(y_test, preds) * 100\n",
    "    f1 = f1_score(y_test, preds) * 100\n",
    "    results.append([name, acc, prec, rec, f1])\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Accuracy (%)\", \"Precision (%)\", \"Recall (%)\", \"F1-Score (%)\"])\n",
    "\n",
    "# Round to 2 decimal places\n",
    "df_results = df_results.round(2)\n",
    "\n",
    "# Sort by Accuracy\n",
    "df_results = df_results.sort_values(by=\"Accuracy (%)\", ascending=False)\n",
    "\n",
    "print(df_results)\n",
    "\n",
    "# Save comparison table\n",
    "df_results.to_csv(\"classifier_comparison_percentage.csv\", index=False)\n",
    "print(\"\\nComparison saved to 'classifier_comparison_percentage.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b788e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'knn_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     27\u001b[39m     rec = custom_recall(y_true, y_pred)\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m2\u001b[39m * prec * rec / (prec + rec) \u001b[38;5;28;01mif\u001b[39;00m (prec + rec) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     30\u001b[39m models = [\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mKNN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mknn_preds\u001b[49m),\n\u001b[32m     32\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mNaive Bayes\u001b[39m\u001b[33m\"\u001b[39m, nb_preds),\n\u001b[32m     33\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mMultinomialNB\u001b[39m\u001b[33m\"\u001b[39m, multinb_preds),\n\u001b[32m     34\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mBernoulliNB\u001b[39m\u001b[33m\"\u001b[39m, bernoullinb_preds)\n\u001b[32m     35\u001b[39m ]\n\u001b[32m     37\u001b[39m results = []\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, preds \u001b[38;5;129;01min\u001b[39;00m models:\n",
      "\u001b[31mNameError\u001b[39m: name 'knn_preds' is not defined"
     ]
    }
   ],
   "source": [
    "# Comparison table and graph for all classifiers (custom metrics)\n",
    "import numpy as np\n",
    "\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    # Accuracy = (TP + TN) / ALL\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "def custom_precision(y_true, y_pred):\n",
    "    # Precision = TP / (TP + FP)\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "def custom_recall(y_true, y_pred):\n",
    "    # Recall = TP / (TP + FN)\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "def custom_f1(y_true, y_pred):\n",
    "    # F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    prec = custom_precision(y_true, y_pred)\n",
    "    rec = custom_recall(y_true, y_pred)\n",
    "    return 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "\n",
    "models = [\n",
    "    (\"KNN\", knn_preds),\n",
    "    (\"Naive Bayes\", nb_preds),\n",
    "    (\"MultinomialNB\", multinb_preds),\n",
    "    (\"BernoulliNB\", bernoullinb_preds)\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, preds in models:\n",
    "    acc = custom_accuracy(y_test.values, preds)\n",
    "    prec = custom_precision(y_test.values, preds)\n",
    "    rec = custom_recall(y_test.values, preds)\n",
    "    f1 = custom_f1(y_test.values, preds)\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1 Score\": f1\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n",
    "\n",
    "# Bar plot for comparison\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "results_df.set_index(\"Model\")[metrics].plot(kind=\"bar\", figsize=(10,6))\n",
    "plt.title(\"Classifier Performance Comparison (Custom Metrics)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
